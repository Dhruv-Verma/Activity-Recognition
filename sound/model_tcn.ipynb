{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 1296,
     "status": "ok",
     "timestamp": 1601570430822,
     "user": {
      "displayName": "Dhruv Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjPErif29dJTpYr10Kx4Olz-3fBzlJHUINscaF57A=s64",
      "userId": "04055500837937310636"
     },
     "user_tz": -330
    },
    "id": "YxPWN60xnHWF",
    "outputId": "c229db07-8588-4b5b-c3d6-2bb66bf94d78"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           62Gi       719Mi        33Gi       0.0Ki        29Gi        61Gi\r\n",
      "Swap:         8.0Gi       263Mi       7.7Gi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 10625,
     "status": "ok",
     "timestamp": 1601570503201,
     "user": {
      "displayName": "Dhruv Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjPErif29dJTpYr10Kx4Olz-3fBzlJHUINscaF57A=s64",
      "userId": "04055500837937310636"
     },
     "user_tz": -330
    },
    "id": "EVhv2x0omPUX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from statistics import mode\n",
    "from tqdm import tqdm, trange\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import soundtransfer\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from sklearn.utils import shuffle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(lol):\n",
    "    start = lol[0]\n",
    "    for i in range(1, len(lol)):\n",
    "        start = list(start) + list(lol[i])\n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 182202,
     "status": "ok",
     "timestamp": 1601570827794,
     "user": {
      "displayName": "Dhruv Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjPErif29dJTpYr10Kx4Olz-3fBzlJHUINscaF57A=s64",
      "userId": "04055500837937310636"
     },
     "user_tz": -330
    },
    "id": "Y4WQOp9S_tP2"
   },
   "outputs": [],
   "source": [
    "feature_mode = 'time_seg'\n",
    "mode = 'train'\n",
    "\n",
    "# features = []\n",
    "labels = []\n",
    "for i in range(3):\n",
    "#     features_part = pickle.load(open('features/{}/{}_features_part{}.pkl'.format(feature_mode, mode, i+1), 'rb'))\n",
    "#     features.append(features_part)\n",
    "    labels_part = pickle.load(open('features/{}/{}_labels_part{}.pkl'.format(feature_mode, mode, i+1), 'rb'))\n",
    "    labels.append(labels_part)\n",
    "    \n",
    "# features = concat(concat(features))\n",
    "labels = concat(concat(labels))\n",
    "\n",
    "k=0\n",
    "for i in range(len(labels)-k):\n",
    "    if labels[i-k] == []:\n",
    "#         del features[i-k]\n",
    "        del labels[i-k]\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d5lV09hPoyUA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:68: FutureWarning: Pass classes=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29], y=[ 6  6  6 ... 27 27 27] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 1.024599344652964,\n",
       " 1: 1.6625966744006186,\n",
       " 2: 0.4930025226464855,\n",
       " 3: 1.5149665257223397,\n",
       " 4: 0.10798359955796664,\n",
       " 5: 0.85998099809981,\n",
       " 6: 1.8468535223367697,\n",
       " 7: 4.3232528908999495,\n",
       " 8: 5.749882982280174,\n",
       " 9: 3.420425616547335,\n",
       " 10: 0.32435404171853194,\n",
       " 11: 4.866411997736276,\n",
       " 12: 1.3578004105479236,\n",
       " 13: 2.7181760708076497,\n",
       " 14: 5.84366292898403,\n",
       " 15: 4.003235567970205,\n",
       " 16: 1.7869804655029093,\n",
       " 17: 8.778917815211843,\n",
       " 18: 1.5418594226286535,\n",
       " 19: 1.872185935118659,\n",
       " 20: 3.476430159692743,\n",
       " 21: 4.1966569058077114,\n",
       " 22: 0.6601374174727468,\n",
       " 23: 1.031049160671463,\n",
       " 24: 17.061408730158732,\n",
       " 25: 0.44120936914748965,\n",
       " 26: 1.9412913421379387,\n",
       " 27: 6.991016260162602,\n",
       " 28: 2.9795391545391547,\n",
       " 29: 0.9328939517222674}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_flat = np.concatenate(labels)\n",
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            np.unique(labels_flat),\n",
    "                                            labels_flat)\n",
    "class_weights = dict(enumerate(weights))\n",
    "class_name_weights = {np.unique(labels_flat)[key]:class_weights[key] for key in class_weights.keys()}\n",
    "class_name_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S_H-1vfmo2OO"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(labels_flat.reshape((-1,1)))\n",
    "\n",
    "labels = [ohe.transform(np.array(batch).reshape(-1,1)).toarray() for batch in labels]\n",
    "\n",
    "NUM_CLASSES = len(weights)\n",
    "\n",
    "# features = [batch.reshape((-1, sfreq*2, 1)) for batch in features]\n",
    "\n",
    "# X_train = features\n",
    "y_train = labels\n",
    "del labels_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, batch_size, num_batches, ohe, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.indexes = np.arange(1, num_batches+1)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.ohe = ohe\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return num_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        i = self.indexes[index]\n",
    "        return self.__data_generation(i)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(1, num_batches+1)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, i):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        \n",
    "        X_batch, y_batch = pickle.load(open('features/time_seg/train_batches/batch{}.pkl'.format(i), 'rb'))\n",
    "        X_batch = X_batch.reshape((-1, sfreq*2, 1))[:,::4,:]\n",
    "        y_batch = self.ohe.transform(y_batch.reshape((-1,1))).toarray()\n",
    " \n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_fn = os.listdir('features/time_seg/train_batches')\n",
    "num_batches = len(train_batch_fn)\n",
    "batch_size = 64\n",
    "\n",
    "train_generator = DataGenerator(batch_size, num_batches, ohe, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mode = 'time_seg'\n",
    "mode = 'test'\n",
    "\n",
    "test_features = pickle.load(open('features/{}/{}_features.pkl'.format(feature_mode, mode), 'rb'))\n",
    "test_labels = pickle.load(open('features/{}/{}_labels.pkl'.format(feature_mode, mode), 'rb'))\n",
    "\n",
    "test_features = concat(test_features)\n",
    "test_labels = concat(test_labels)\n",
    "\n",
    "k=0\n",
    "for i in range(len(test_features)-k):\n",
    "    if test_labels[i-k] == []:\n",
    "        del test_features[i-k]\n",
    "        del test_labels[i-k]\n",
    "        k+=1\n",
    "        \n",
    "test_features = np.concatenate(test_features).astype('float64')\n",
    "test_labels = np.concatenate(test_labels).astype('int64')\n",
    "\n",
    "test_labels = ohe.transform(test_labels.reshape((-1,1))).toarray()\n",
    "test_features = test_features.reshape((-1, sfreq*2, 1))\n",
    "\n",
    "X_test = test_features\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test  shape (23657, 32000, 1)\n",
      "y test  shape (23657, 30)\n"
     ]
    }
   ],
   "source": [
    "# print('X train shape {}'.format(X_train.shape))\n",
    "print('X test  shape {}'.format(X_test.shape))\n",
    "# print('y train shape {}'.format(y_train.shape))\n",
    "print('y test  shape {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 8000, 1)]         0         \n",
      "_________________________________________________________________\n",
      "matching_conv1D (Conv1D)     (None, 8000, 128)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_0 (Conv1D)            (None, 8000, 128)         384       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_12 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_1 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "matching_identity (Lambda)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_0 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_14 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_1 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_15 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "matching_identity (Lambda)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_0 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_16 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_1 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_17 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "matching_identity (Lambda)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_0 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_18 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_1 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_19 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "matching_identity (Lambda)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_0 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_20 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_1 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_21 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "matching_identity (Lambda)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_0 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_22 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1D_1 (Conv1D)            (None, 8000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_23 (Spatia (None, 8000, 128)         0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 443,934\n",
      "Trainable params: 443,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "timesteps = sfreq * 2 // 4\n",
    "input_dim = 1\n",
    "\n",
    "i = Input(shape=(timesteps, input_dim))\n",
    "# o = TCN(return_sequences=True, nb_filters=128)(i)\n",
    "o = TCN(return_sequences=False, nb_filters=128)(i)\n",
    "o = Dense(512, activation='relu')(o)\n",
    "o = Dropout(0.4)(o)\n",
    "o = Dense(NUM_CLASSES, activation='sigmoid')(o)\n",
    "model = Model(inputs=[i], outputs=[o])\n",
    "\n",
    "# model.summary()\n",
    "tcn_full_summary(model)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "checkpoint=ModelCheckpoint('checkpoints/model.h5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "earlystopping=EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "reducelr=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_lr=0.000001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_test[:,::4,:], y_test, epochs=100, batch_size=64,\n",
    "                    class_weight=class_weights, callbacks= [checkpoint, earlystopping, reducelr], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1346 - categorical_accuracy: 0.1258WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1346 - categorical_accuracy: 0.1258\n",
      "Epoch 00001: val_categorical_accuracy improved from 0.15205 to 0.21118, saving model to checkpoints/model.h5\n",
      "2687/2687 [==============================] - 1460s 543ms/step - loss: 0.1346 - categorical_accuracy: 0.1258 - val_loss: 0.1247 - val_categorical_accuracy: 0.2112\n",
      "Epoch 2/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1307 - categorical_accuracy: 0.1453WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1307 - categorical_accuracy: 0.1453\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.21118\n",
      "2687/2687 [==============================] - 1461s 544ms/step - loss: 0.1307 - categorical_accuracy: 0.1453 - val_loss: 0.1291 - val_categorical_accuracy: 0.1632\n",
      "Epoch 3/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1284 - categorical_accuracy: 0.1555WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1284 - categorical_accuracy: 0.1555\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.21118 to 0.23139, saving model to checkpoints/model.h5\n",
      "2687/2687 [==============================] - 1459s 543ms/step - loss: 0.1284 - categorical_accuracy: 0.1555 - val_loss: 0.1273 - val_categorical_accuracy: 0.2314\n",
      "Epoch 4/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1264 - categorical_accuracy: 0.1694WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1264 - categorical_accuracy: 0.1694\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.23139 to 0.24014, saving model to checkpoints/model.h5\n",
      "2687/2687 [==============================] - 1463s 545ms/step - loss: 0.1264 - categorical_accuracy: 0.1694 - val_loss: 0.1245 - val_categorical_accuracy: 0.2401\n",
      "Epoch 5/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1247 - categorical_accuracy: 0.1730WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1247 - categorical_accuracy: 0.1730\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.24014\n",
      "2687/2687 [==============================] - 1459s 543ms/step - loss: 0.1247 - categorical_accuracy: 0.1730 - val_loss: 0.1329 - val_categorical_accuracy: 0.1470\n",
      "Epoch 6/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1232 - categorical_accuracy: 0.1764WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1232 - categorical_accuracy: 0.1764\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.24014\n",
      "2687/2687 [==============================] - 1461s 544ms/step - loss: 0.1232 - categorical_accuracy: 0.1764 - val_loss: 0.1276 - val_categorical_accuracy: 0.1969\n",
      "Epoch 7/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1218 - categorical_accuracy: 0.1795WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1218 - categorical_accuracy: 0.1795\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.24014\n",
      "2687/2687 [==============================] - 1461s 544ms/step - loss: 0.1218 - categorical_accuracy: 0.1795 - val_loss: 0.1281 - val_categorical_accuracy: 0.1906\n",
      "Epoch 8/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1207 - categorical_accuracy: 0.1804WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1207 - categorical_accuracy: 0.1804\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.24014\n",
      "2687/2687 [==============================] - 1462s 544ms/step - loss: 0.1207 - categorical_accuracy: 0.1804 - val_loss: 0.1305 - val_categorical_accuracy: 0.1671\n",
      "Epoch 9/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1193 - categorical_accuracy: 0.1789WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1193 - categorical_accuracy: 0.1789\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.24014\n",
      "2687/2687 [==============================] - 1463s 544ms/step - loss: 0.1193 - categorical_accuracy: 0.1789 - val_loss: 0.1320 - val_categorical_accuracy: 0.1647\n",
      "Epoch 10/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1178 - categorical_accuracy: 0.1812WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1178 - categorical_accuracy: 0.1812\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.24014\n",
      "2687/2687 [==============================] - 1462s 544ms/step - loss: 0.1178 - categorical_accuracy: 0.1812 - val_loss: 0.1281 - val_categorical_accuracy: 0.1840\n",
      "Epoch 11/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "2686/2687 [============================>.] - ETA: 0s - loss: 0.1172 - categorical_accuracy: 0.1828WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2687/2687 [==============================] - ETA: 0s - loss: 0.1173 - categorical_accuracy: 0.1828\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.24014\n",
      "2687/2687 [==============================] - 1460s 544ms/step - loss: 0.1173 - categorical_accuracy: 0.1828 - val_loss: 0.1294 - val_categorical_accuracy: 0.1886\n",
      "Epoch 12/100\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "1790/2687 [==================>...........] - ETA: 7:44 - loss: 0.1158 - categorical_accuracy: 0.1853"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                  validation_data=(X_test[:,::4,:], y_test),\n",
    "                  class_weight=class_weights,\n",
    "                    epochs=100,\n",
    "                  callbacks=[checkpoint, earlystopping, reducelr],\n",
    "                  shuffle=True,\n",
    "                  use_multiprocessing=True,\n",
    "                  workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "max_acc = 0\n",
    "for epoch in range(100):\n",
    "    # Training\n",
    "    num_batches = len(train_batch_fn)\n",
    "    t = trange(num_batches, desc='epoch {}: Train'.format(epoch+1))\n",
    "\n",
    "    for i in t:\n",
    "        X_batch, y_batch = pickle.load(open('features/time_seg/train_batches/batch{}.pkl'.format(i+1), 'rb'))\n",
    "        X_batch = X_batch.reshape((-1, sfreq*2, 1))\n",
    "        y_batch = ohe.transform(y_batch.reshape((-1,1))).toarray()\n",
    "        \n",
    "        mini_batch_size = 8\n",
    "        for a in range(batch_size//mini_batch_size):\n",
    "            X = X_batch[a*mini_batch_size: (a+1)*mini_batch_size]\n",
    "            y = y_batch[a*mini_batch_size: (a+1)*mini_batch_size]\n",
    "            metrics = model.train_on_batch(X, y, class_weight=class_weights, reset_metrics=False, return_dict=True)\n",
    "            t.set_postfix(metrics)\n",
    "  \n",
    "    history['train'].append(metrics)\n",
    "    \n",
    "    # Testing\n",
    "#     X = X_test[:,::4,:]\n",
    "    metrics = model.test_on_batch(X_test, y_test, reset_metrics=False, return_dict=True)\n",
    "#     print(\"epoch {} val metrics: {}\".format(epoch+1, metrics))\n",
    "#     num_batches = len(X_test) \n",
    "#     t = trange(num_batches, desc='epoch {}: Test'.format(epoch+1))\n",
    "#     for i in t:\n",
    "#         X_batch = X_test[i]\n",
    "#         y_batch = y_test[i]\n",
    "#         num_samples = len(X_batch)\n",
    "#         for j in range(num_samples):\n",
    "#             X = X_batch[j:j+1]\n",
    "#             y = y_batch[j:j+1]\n",
    "#             lstm = model.get_layer('lstm')\n",
    "#         lstm.reset_states()\n",
    "    t.set_postfix(\"val metrics: {}\".format(metrics))\n",
    "    history['test'].append(metrics)\n",
    "    \n",
    "    if max_acc < metrics['categorical_accuracy']:\n",
    "        print('val accuracy improved from {} to {}. saving model...'.format(max_acc, metrics['categorical_accuracy']))\n",
    "        max_acc = metrics['categorical_accuracy']\n",
    "        model.save('checkpoints/model_tcn.h5')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoints/model_{}.h5'.format(sfreq))\n",
    "print('Loaded best checkpoint!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = ohe.inverse_transform(y_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {label: ohe.transform(np.array(label).reshape(-1,1)).toarray() for label in np.unique(labels_test)}\n",
    "\n",
    "contexts = {\n",
    "    'monolithic': ['alarm-clock', 'baby-cry', 'blender', 'car-horn', 'cat-meow',\n",
    "       'chopping', 'cooking', 'cough', 'dishwasher', 'dog-bark', 'door',\n",
    "       'doorbell', 'drill', 'engine', 'flush', 'hair-dryer', 'hammer',\n",
    "       'hazard-alarm', 'knock', 'laugh', 'microwave', 'phone-ring', 'saw',\n",
    "       'shaver', 'snore', 'speech', 'toothbrush', 'typing', 'vacuum',\n",
    "       'water-running'],\n",
    "    'bathroom': ['water-running', 'shaver', 'toothbrush', 'flush', 'hair-dryer'],\n",
    "    'kitchen': ['hazard-alarm', 'speech', 'chopping', 'water-running', 'microwave', 'blender', 'dishwasher', 'cooking'],\n",
    "    'bedroom': ['speech', 'baby-cry', 'cough', 'snore', 'alarm-clock'],\n",
    "    'office': ['phone-ring', 'speech', 'cough', 'door', 'knock', 'typing'],\n",
    "    'entrance': ['speech', 'door', 'knock', 'doorbell', 'laugh'],\n",
    "    'workshop': ['drill', 'hazard-alarm', 'speech', 'vacuum', 'hammer', 'saw'],\n",
    "    'outdoor': ['dog-bark', 'hazard-alarm', 'speech', 'car-horn', 'engine', 'cat-meow']\n",
    "}\n",
    "\n",
    "context_encoding = {}\n",
    "for context in contexts.keys():\n",
    "    context_encoding[context] = np.zeros((1,30))\n",
    "    for activity in contexts[context]:\n",
    "        context_encoding[context] += encoding[activity]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context in contexts.keys():\n",
    "#     context = list(contexts.keys())[0]\n",
    "    valid_idx = [i for i, label in enumerate(labels_test) if label in contexts[context]]\n",
    "    valid_pred = pred[valid_idx]\n",
    "    valid_labels = labels_test[valid_idx]\n",
    "    valid_pred = np.argmax(valid_pred * context_encoding[context], axis=1)\n",
    "    valid_pred = [ohe.categories_[0][pr] for pr in valid_pred]\n",
    "    acc = accuracy_score(valid_labels, valid_pred)\n",
    "    f1 = f1_score(valid_labels, valid_pred, average='weighted')\n",
    "    conf_mat = confusion_matrix(valid_labels, valid_pred, normalize='true')\n",
    "    label_names = np.unique(valid_labels)\n",
    "    print('{}, {}, {}, {}'.format(sfreq, context, acc, f1))\n",
    "    # fig, ax = plt.subplots(figsize=(13,10))\n",
    "    # sns.heatmap(conf_mat, cmap='Blues', ax=ax, xticklabels=label_names, yticklabels=label_names, annot=False)\n",
    "    # ax.set_title('sFreq: {}, Context: {}, Acc: {}'.format(sfreq,context,acc))\n",
    "    # ax.set_xlabel('Predicted Label')\n",
    "    # ax.set_ylabel('True Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File-Level Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'wild'\n",
    "test_features = pickle.load(open('features/{}_features.pkl'.format(mode), 'rb'))\n",
    "test_labels = pickle.load(open('features/{}_labels.pkl'.format(mode), 'rb'))\n",
    "\n",
    "test_features_flat = []\n",
    "for cls in test_features:\n",
    "    for file in cls:\n",
    "        if len(file) != 0:\n",
    "            test_features_flat.append(file)\n",
    "\n",
    "test_labels_ohe = []\n",
    "for cls in test_labels:\n",
    "    for file in cls:\n",
    "        if len(file) != 0:\n",
    "            file_labels = ohe.transform(np.array(file).reshape((-1,1))).toarray()\n",
    "            test_labels_ohe.append(file_labels)\n",
    "\n",
    "NUM_CLASSES = 30\n",
    "\n",
    "X_test = [file.reshape((-1, vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS, 1)) for file in test_features_flat] \n",
    "y_test = [file.reshape((-1, NUM_CLASSES)) for file in test_labels_ohe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoints/model.h5')\n",
    "print('Loaded best checkpoint!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = np.array([ohe.inverse_transform(file).flatten() for file in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([model.predict(file) for file in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_int = soundtransfer.label\n",
    "label_inv = soundtransfer.label_inv\n",
    "\n",
    "encoding = {label: ohe.transform([[label]]).toarray() for label in ohe.categories_[0]}\n",
    "\n",
    "contexts = {\n",
    "    'monolithic': ['alarm-clock', 'baby-cry', 'blender', 'car-horn', 'cat-meow',\n",
    "       'chopping', 'cooking', 'cough', 'dishwasher', 'dog-bark', 'door',\n",
    "       'doorbell', 'drill', 'engine', 'flush', 'hair-dryer', 'hammer',\n",
    "       'hazard-alarm', 'knock', 'laugh', 'microwave', 'phone-ring', 'saw',\n",
    "       'shaver', 'snore', 'speech', 'toothbrush', 'typing', 'vacuum',\n",
    "       'water-running'],\n",
    "    'bathroom': ['water-running', 'shaver', 'toothbrush', 'flush', 'hair-dryer'],\n",
    "    'kitchen': ['hazard-alarm', 'speech', 'chopping', 'water-running', 'microwave', 'blender', 'dishwasher', 'cooking'],\n",
    "    'bedroom': ['speech', 'baby-cry', 'cough', 'snore', 'alarm-clock'],\n",
    "    'office': ['phone-ring', 'speech', 'cough', 'door', 'knock', 'typing'],\n",
    "    'entrance': ['speech', 'door', 'knock', 'doorbell', 'laugh'],\n",
    "    'workshop': ['drill', 'hazard-alarm', 'speech', 'vacuum', 'hammer', 'saw'],\n",
    "    'outdoor': ['dog-bark', 'hazard-alarm', 'speech', 'car-horn', 'engine', 'cat-meow']\n",
    "}\n",
    "\n",
    "context_encoding = {}\n",
    "for context in contexts.keys():\n",
    "    context_encoding[context] = np.zeros((1,30))\n",
    "    for activity in contexts[context]:\n",
    "        context_encoding[context] += encoding[label_int[activity]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context in contexts.keys():\n",
    "    valid_idx = [i for i, label in enumerate(labels_test) if label[0] in [label_int[x] for x in contexts[context]]]\n",
    "    valid_pred = pred[valid_idx]\n",
    "    valid_labels = labels_test[valid_idx]\n",
    "    valid_pred = [file * context_encoding[context] for file in valid_pred]\n",
    "\n",
    "    # Transform to per file level labels\n",
    "    valid_pred = [np.argmax(np.sum(file, axis=0)) for file in valid_pred]\n",
    "    valid_labels = [file[0] for file in valid_labels]\n",
    "\n",
    "    acc = accuracy_score(valid_labels, valid_pred)\n",
    "    f1 = f1_score(valid_labels, valid_pred, average='weighted')\n",
    "    conf_mat = confusion_matrix(valid_labels, valid_pred, normalize='true')\n",
    "    # label_names = np.unique(valid_labels)\n",
    "    print('{},{},{}'.format(context, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZ3nl0I+mVkmJRBp9GYBH+",
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
